{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression with Newton's Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Written by: Dylan Hematillake; What is Logistic Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is named after the logistic function. In linear regression, the dependent variable is continuous. It can have any one of the infinite number of possible values. Logistic Regression is when the response variable is categorical in nature, a discrete outcome.\n",
    "\n",
    "The logistic function is also the sigmoid function as it is a S-shaped curve that takes a real value and maps it to a value between 0 and 1 but never exactly 0 and 1.\n",
    "\n",
    "$$\\sigma = \\frac{1}{1+e^{-x}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f60a7e37f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from patsy import dmatrices\n",
    "import warnings\n",
    "\n",
    "def sigmoid(x):\n",
    "    response = 1/(1+np.exp(-x))\n",
    "    return response\n",
    "\n",
    "x = np.linspace(-10,10,num=100)\n",
    "plt.plot(x,sigmoid(x),color='purple')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The central premise of Logistic Regression is the assumption that your input space can be separated into two nice regions for each class by a linear boundary. The data must be seperable in n dimensions.\n",
    "\n",
    "So, given a function:\n",
    "\n",
    "$$\\beta_o + \\beta_1 x_1+\\beta_2 x_2$$\n",
    "\n",
    "To take a function that goes from $[-\\infty,\\infty]$ to a probability that goes from $[0,1]$ we use the odds function.\n",
    "\n",
    "Let $P(X)$ denote the probability of an event $X$ occuring. The odds ratio, $OR(X)$ is defined as:\n",
    "\n",
    "$$\\frac{P(X)}{1-P(X)}$$\n",
    "\n",
    "We can then define a general model:\n",
    "\n",
    "$$logit(p_i)=log(\\frac{p_i}{1-p_i})=\\beta_o+\\sum_{j=1}^J \\beta_j x_{ji}$$\n",
    "\n",
    "Where $p_i$ is the probability of event i, $\\beta_o$ is the intercept, \\beta_j are the coefficients (factors) and $x_{ji}$ are the variables of the factors.\n",
    "\n",
    "Odds are used since the equation gives can give the largest range of coefficients such that the output is $[0,1]$.\n",
    "\n",
    "The above boundary function still ranges from $[-\\infty,\\infty]$. So, we take the log of $OR(X)$, called the log-odds function as $log(OR(X))$ goes from $[-\\infty,\\infty]$ as $OR(X)$ goes from $[0,\\infty]$\n",
    "\n",
    "The impact of logistic regression is that we can no longer understand the prediction as a linear combination of the inputs as we can with linear regression. \n",
    "\n",
    "To calculate the boundary function, we calculate the Maximum Likelihood Estimation, MLE. In deep learning, MLE is backpropagation, defined as:\n",
    "$$L(\\theta(x))=f(X(\\theta))$$\n",
    "\n",
    "Newton's method can be used to find the minima and maxima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why use Newton's Method for Optimization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  Newton's method converges faster than gradient descent when maximizing logistic regression log likelihood\n",
    "\n",
    "-  Each iteration is more expensive than gradient descent because of calculating the inverse Hessian, defined as:\n",
    "\n",
    "$${\\mathbf  H}={\\begin{bmatrix}{\\dfrac  {\\partial ^{2}f}{\\partial x_{1}^{2}}}&{\\dfrac  {\\partial ^{2}f}{\\partial x_{1}\\,\\partial x_{2}}}&\\cdots &{\\dfrac  {\\partial ^{2}f}{\\partial x_{1}\\,\\partial x_{n}}}\\\\[2.2ex]{\\dfrac  {\\partial ^{2}f}{\\partial x_{2}\\,\\partial x_{1}}}&{\\dfrac  {\\partial ^{2}f}{\\partial x_{2}^{2}}}&\\cdots &{\\dfrac  {\\partial ^{2}f}{\\partial x_{2}\\,\\partial x_{n}}}\\\\[2.2ex]\\vdots &\\vdots &\\ddots &\\vdots \\\\[2.2ex]{\\dfrac  {\\partial ^{2}f}{\\partial x_{n}\\,\\partial x_{1}}}&{\\dfrac  {\\partial ^{2}f}{\\partial x_{n}\\,\\partial x_{2}}}&\\cdots &{\\dfrac  {\\partial ^{2}f}{\\partial x_{n}^{2}}}\\end{bmatrix}}.$$\n",
    "\n",
    "- As long as the data points are not very large, Newton's method is preferred for MLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization must be done as well, and is important in machine learning to prevent overfitting. A regularization term prevents coefficients to fit so perfectly as to overfit. L1 and L2 regularization vary be the method of norm they employ.\n",
    "\n",
    "L1 Regularization of Least Squares:\n",
    "$$\\textbf{w*}=arg\\min{\\textbf{w}}\\sum_j \\big(t(\\textbf{x}_j-\\sum_i w_i h_i(\\textbf{x}_j)\\big)^2 + \\lambda\\sum_{i=1}^k|w_i|$$\n",
    "\n",
    "L2 Regularization of Least Squares:\n",
    "$$\\textbf{w*}=arg\\min{\\textbf{w}}\\sum_j \\big(t(\\textbf{x}_j-\\sum_i w_i h_i(\\textbf{x}_j)\\big)^2 + \\lambda\\sum_{i=1}^k w_i^2$$\n",
    "\n",
    "- L2 regularization is computationally efficient due to having analytic solutions while L1 is inefficient on non-sparse cases\n",
    "- L2 regularization has non-sparse outputs while L1 has sparse outputs\n",
    "- L2 regularization has no feature selection while L1 has built-in feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create play data\n",
    "\n",
    "#set seed for reproducability \n",
    "np.random.seed(0)\n",
    "\n",
    "#Define hyperparameters (model parameters)\n",
    "#when to stop\n",
    "tol = 1e-8\n",
    "lam = None #12 regularization\n",
    "max_iters=20\n",
    "r = 0.95 #covariance between x and y, measure how two variables move together\n",
    "n = 1000 #umber of observations in dataset to generate\n",
    "sigma = 1 #variance of noise - data spead\n",
    "\n",
    "#model settings\n",
    "beta_x,beta_z,beta_v = -4,.9,1 #true beta coeff\n",
    "var_x,var_z,var_v = 1, 1, 4 #variance of inpus\n",
    "\n",
    "#model specification you want to fit\n",
    "formula = 'y ~ x + z + v + np.exp(x) + I(v**2 + z)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Intercept</th>\n",
       "      <th>x</th>\n",
       "      <th>z</th>\n",
       "      <th>v</th>\n",
       "      <th>np.exp(x)</th>\n",
       "      <th>I(v ** 2 + z)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.805133</td>\n",
       "      <td>-1.678592</td>\n",
       "      <td>-230.536312</td>\n",
       "      <td>0.164453</td>\n",
       "      <td>53145.312449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.320743</td>\n",
       "      <td>-0.612110</td>\n",
       "      <td>-321.120883</td>\n",
       "      <td>0.266937</td>\n",
       "      <td>103118.009125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.689545</td>\n",
       "      <td>-1.998587</td>\n",
       "      <td>0.006285</td>\n",
       "      <td>0.184604</td>\n",
       "      <td>-1.998547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.914205</td>\n",
       "      <td>-0.962069</td>\n",
       "      <td>-56.335960</td>\n",
       "      <td>0.400835</td>\n",
       "      <td>3172.778273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.036999</td>\n",
       "      <td>0.166842</td>\n",
       "      <td>-0.033775</td>\n",
       "      <td>1.037692</td>\n",
       "      <td>0.167983</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Intercept         x         z           v  np.exp(x)  I(v ** 2 + z)\n",
       "0        1.0 -1.805133 -1.678592 -230.536312   0.164453   53145.312449\n",
       "1        1.0 -1.320743 -0.612110 -321.120883   0.266937  103118.009125\n",
       "2        1.0 -1.689545 -1.998587    0.006285   0.184604      -1.998547\n",
       "3        1.0 -0.914205 -0.962069  -56.335960   0.400835    3172.778273\n",
       "4        1.0  0.036999  0.166842   -0.033775   1.037692       0.167983"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#generate and organize data\n",
    "#multvariate normal is a generalized normal distribution to higher dimensions\n",
    "#stochastic process (random). Define 3 random variables where x,z are related closely\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "x,z=np.random.multivariate_normal([0,0],[[var_x,r],[r,var_z]],n).T\n",
    "v=np.random.normal(0,var_v,n)**3\n",
    "A = pd.DataFrame({'x' : x,'z': z,'v' : v})\n",
    "A['Log_Odds']=sigmoid(A[['x','z','v']].dot([beta_x,beta_z,beta_v])+sigma*np.random.normal(0,1,n))\n",
    "\n",
    "#binomial random variable is the number of successes x in n repeated trails of a binomial experiment\n",
    "A['y'] = [np.random.binomial(1,p) for p in A.Log_Odds]\n",
    "\n",
    "y, X= dmatrices(formula,A,return_type='dataframe')\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Algorithm setup\n",
    "\n",
    "def catch_singularity(f):\n",
    "    def silencer(*args,**kwargs):\n",
    "        try:\n",
    "            return f(*args,**kwargs)\n",
    "        except np.linalg.LinAlgError:\n",
    "            warnings.warn('Algorithm terminated - singular Hessian')\n",
    "            return args[0]\n",
    "    return silencer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Newton Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Newton's method, a function $f(\\beta)$ iteratively computs the following estimate:\n",
    "\n",
    "$$\\beta^+ = \\beta - Hf(\\beta)^{-1}\\nabla f(\\beta)$$\n",
    "\n",
    "The Hessian of the log-likelihood for logistic regresion is given by:\n",
    "\n",
    "$$Hf(\\beta)=-X^TWX$$\n",
    "\n",
    "With the gradient:\n",
    "$$\\nabla f(\\beta)=X^T(y-p)$$\n",
    "Where:\n",
    "$$W := diag(p(1-p))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relation to Iteratively Reweighted Least Squares (IRLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For logistic regression, this step is equivalent to computing a weighted least squares estimator at each iteration where the method of least squares for estimating parameters by minimizing squared discrepancies between observed data and their expected values:\n",
    "\n",
    "$$\\beta^+=arg\\min_\\beta (z-X\\beta)^TW(z-X\\beta)$$\n",
    "\n",
    "With $W$ as before and the adjusted response $z$ given by:\n",
    "\n",
    "$$z := X\\beta+W^{-1}(y-p)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "@catch_singularity\n",
    "\n",
    "def newton_step(curr,X,lam=None):\n",
    "    #One naive step\n",
    "    p=np.array(sigmoid(X.dot(curr[:,0])), ndmin=2).T\n",
    "    W = np.diag((p*(1-p))[:,0])\n",
    "    #hessian\n",
    "    hessian = X.T.dot(W).dot(X)\n",
    "    #gradient\n",
    "    grad=X.T.dot(y-p)\n",
    "    \n",
    "    #regularization\n",
    "    if lam:\n",
    "        #return least-squares solution to a linear matrix eq. avoid calculating hessian to reduce computational expense\n",
    "        step,*_=np.linalg.lstsq(hessian+lam*np.eye(curr,shape[0]),grad)\n",
    "    else:\n",
    "        step,*_=np.linalg.lstsq(hessian,grad)\n",
    "        \n",
    "    #update\n",
    "    beta=curr+step\n",
    "    \n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convergence(beta_old,beta_new,tol,iters):\n",
    "    coef_change=np.abs(beta_old-beta_new)\n",
    "    \n",
    "    #if change hasn't reached threshold, keep training\n",
    "    return not (np.any(coef_change>tol) & (iters < max_iters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical Example\n",
    "#### Standard Newton with Coefficient Convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations : 18\n",
      "Beta: [[ 26321162.0240595 ]\n",
      " [ -6345604.88747682]\n",
      " [-11270766.29545255]\n",
      " [ -9577280.68635647]\n",
      " [ 20672279.24612887]\n",
      " [ -7783646.80695781]]\n"
     ]
    }
   ],
   "source": [
    "#intial conditions\n",
    "\n",
    "beta_old, beta=np.ones((len(X.columns),1)),np.zeros((len(X.columns),1))\n",
    "\n",
    "#iterations\n",
    "iters = 0\n",
    "converged= False\n",
    "\n",
    "while not converged:\n",
    "    beta_old = beta\n",
    "    beta = newton_step(beta,X,lam=lam)\n",
    "    iters += 1\n",
    "    converged = convergence(beta_old,beta,tol,iters)\n",
    "\n",
    "print('Iterations : {}'.format(iters))\n",
    "print('Beta: {}'.format(beta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
